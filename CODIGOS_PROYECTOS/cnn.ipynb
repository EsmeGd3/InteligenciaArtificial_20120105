{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:12:59.231970Z",
     "start_time": "2018-11-08T00:12:51.800950Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:13:12.550878Z",
     "start_time": "2018-11-08T00:12:59.234748Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "#from tensorflow.keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, Conv2D\n",
    ")\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar set de Imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.248080Z",
     "start_time": "2018-11-08T00:13:12.553292Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirname = os.path.join(os.getcwd(),'C:\\\\Users\\\\FAS\\\\Desktop\\\\alcaraz\\\\InteligenciaArtificial_20120105\\\\DATASET_CNN\\\\categorias')\n",
    "imgpath = dirname + os.sep \n",
    "\n",
    "images = []\n",
    "directories = []\n",
    "dircount = []\n",
    "prevRoot=''\n",
    "cant=0\n",
    "\n",
    "print(\"leyendo imagenes de \",imgpath)\n",
    "\n",
    "for root, dirnames, filenames in os.walk(imgpath):\n",
    "    for filename in filenames:\n",
    "        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
    "            cant=cant+1\n",
    "            filepath = os.path.join(root, filename)\n",
    "            image = plt.imread(filepath)\n",
    "            if(len(image.shape)==3):\n",
    "                \n",
    "                images.append(image)\n",
    "            b = \"Leyendo imagen...\" + str(cant)\n",
    "            print (b, end=\"\\r\")\n",
    "            if prevRoot !=root:\n",
    "                print(root, cant)\n",
    "                prevRoot=root\n",
    "                directories.append(root)\n",
    "                dircount.append(cant)\n",
    "                cant=0\n",
    "dircount.append(cant)\n",
    "\n",
    "dircount = dircount[1:]\n",
    "dircount[0]=dircount[0]+1\n",
    "print('Directorios leidos:',len(directories))\n",
    "print(\"Imagenes en cada directorio\", dircount)\n",
    "print('suma Total de imagenes en subdirs:',sum(dircount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.269861Z",
     "start_time": "2018-11-08T00:16:45.251786Z"
    }
   },
   "outputs": [],
   "source": [
    "labels=[]\n",
    "indice=0\n",
    "for cantidad in dircount:\n",
    "    for i in range(cantidad):\n",
    "        labels.append(indice)\n",
    "    indice=indice+1\n",
    "print(\"Cantidad etiquetas creadas: \",len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.285925Z",
     "start_time": "2018-11-08T00:16:45.273489Z"
    }
   },
   "outputs": [],
   "source": [
    "Incidentes=[]\n",
    "indice=0\n",
    "for directorio in directories:\n",
    "    name = directorio.split(os.sep)\n",
    "    print(indice , name[len(name)-1])\n",
    "    Incidentes.append(name[len(name)-1])\n",
    "    indice=indice+1\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    print(f\"Image {i} shape: {np.array(img).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.498672Z",
     "start_time": "2018-11-08T00:16:45.290061Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.array(labels)\n",
    "X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
    "\n",
    "\n",
    "\n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos Sets de Entrenamiento y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.669596Z",
     "start_time": "2018-11-08T00:16:45.502716Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X,test_X,train_Y,test_Y = train_test_split(X,y,test_size=0.2)\n",
    "print('Training data shape : ', train_X.shape, train_Y.shape)\n",
    "print('Testing data shape : ', test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:49.319746Z",
     "start_time": "2018-11-08T00:16:45.673944Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(train_Y[0]))\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(test_Y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamos las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:50.798162Z",
     "start_time": "2018-11-08T00:16:49.322999Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X/255.\n",
    "test_X = test_X/255.\n",
    "plt.imshow(test_X[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacemos el One-hot Encoding para la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:50.815482Z",
     "start_time": "2018-11-08T00:16:50.800831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_Y[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])\n",
    "print(len(train_X))\n",
    "print(len(train_Y_one_hot))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos el Set de Entrenamiento y Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.218520Z",
     "start_time": "2018-11-08T00:16:50.818992Z"
    }
   },
   "outputs": [],
   "source": [
    "#Mezclar todo y crear los grupos de entrenamiento y testing\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.228116Z",
     "start_time": "2018-11-08T00:16:51.222460Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos el modelo de CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.244776Z",
     "start_time": "2018-11-08T00:16:51.238704Z"
    }
   },
   "outputs": [],
   "source": [
    "#declaramos variables con los parámetros de configuración de la red\n",
    "INIT_LR = 1e-3 # Valor inicial de learning rate. El valor 1e-3 corresponde con 0.001\n",
    "epochs = 20 # Cantidad de iteraciones completas al conjunto de imagenes de entrenamiento\n",
    "batch_size = 64 # cantidad de imágenes que se toman a la vez en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.384131Z",
     "start_time": "2018-11-08T00:16:51.252188Z"
    }
   },
   "outputs": [],
   "source": [
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(28,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(Flatten())\n",
    "sport_model.add(Dense(32, activation='linear'))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.401674Z",
     "start_time": "2018-11-08T00:16:51.386676Z"
    }
   },
   "outputs": [],
   "source": [
    "sport_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.472349Z",
     "start_time": "2018-11-08T00:16:51.406817Z"
    }
   },
   "outputs": [],
   "source": [
    "sport_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamos el modelo: Aprende a clasificar imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:49.562522Z",
     "start_time": "2018-11-08T00:16:51.474807Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# este paso puede tomar varios minutos, dependiendo de tu ordenador, cpu y memoria ram libre\n",
    "sport_train = sport_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:49.676566Z",
     "start_time": "2018-11-08T00:20:49.566203Z"
    }
   },
   "outputs": [],
   "source": [
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "sport_model.save(\"C:\\\\Users\\\\FAS\\\\Desktop\\\\alcaraz\\\\InteligenciaArtificial_20120105\\\\red.h5\")\n",
    "sport_model.save(\"C:\\\\Users\\\\FAS\\\\Desktop\\\\alcaraz\\\\InteligenciaArtificial_20120105\\\\red.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluamos la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:54.462929Z",
     "start_time": "2018-11-08T00:20:49.678643Z"
    }
   },
   "outputs": [],
   "source": [
    "test_eval = sport_model.evaluate(test_X, test_Y_one_hot, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:54.474683Z",
     "start_time": "2018-11-08T00:20:54.465378Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_train.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:55.014647Z",
     "start_time": "2018-11-08T00:20:54.479693Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = sport_train.history['accuracy']\n",
    "val_accuracy = sport_train.history['val_accuracy']\n",
    "loss = sport_train.history['loss']\n",
    "val_loss = sport_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:58.050602Z",
     "start_time": "2018-11-08T00:20:55.021862Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes2 = sport_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:58.262575Z",
     "start_time": "2018-11-08T00:20:58.052878Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes=[]\n",
    "for predicted_sport in predicted_classes2:\n",
    "    predicted_classes.append(predicted_sport.tolist().index(max(predicted_sport)))\n",
    "predicted_classes=np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:58.272559Z",
     "start_time": "2018-11-08T00:20:58.264703Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendamos de los errores: Qué mejorar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:59.822110Z",
     "start_time": "2018-11-08T00:20:58.275464Z"
    }
   },
   "outputs": [],
   "source": [
    "correct = np.where(predicted_classes==test_Y)[0]\n",
    "print(\"Found %d correct labels\" % len(correct))\n",
    "for i, correct in enumerate(correct[0:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_X[correct].reshape(28,28,3), cmap='gray', interpolation='none')\n",
    "    plt.title(\"{}, {}\".format(Incidentes[predicted_classes[correct]],\n",
    "                                                    Incidentes[test_Y[correct]]))\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:21:00.942267Z",
     "start_time": "2018-11-08T00:20:59.829572Z"
    }
   },
   "outputs": [],
   "source": [
    "incorrect = np.where(predicted_classes!=test_Y)[0]\n",
    "print(\"Found %d incorrect labels\" % len(incorrect))\n",
    "for i, incorrect in enumerate(incorrect[0:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_X[incorrect].reshape(28,28,3), cmap='gray', interpolation='none')\n",
    "    plt.title(\"{}, {}\".format(Incidentes[predicted_classes[incorrect]],\n",
    "                                                    Incidentes[test_Y[incorrect]]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:21:00.968727Z",
     "start_time": "2018-11-08T00:21:00.947262Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_names = [\"Class {}\".format(i) for i in range(nClasses)]  \n",
    "print(classification_report(test_Y, predicted_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000233B0752AC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000233B0752AC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from skimage.transform import resize\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt  # Importar matplotlib.pyplot\n",
    "# Cargar el modelo (asegúrate de tener la ruta correcta al modelo)\n",
    "sport_model = load_model('C:\\\\Users\\\\FAS\\\\Desktop\\\\alcaraz\\\\InteligenciaArtificial_20120105\\\\red.h5')  # Sustituye con la ruta a tu modelo\n",
    "\n",
    "# Inicializar lista para almacenar imágenes redimensionadas\n",
    "images = []\n",
    "# Lista de archivos de imagen\n",
    "filenames = ['C:\\\\Users\\\\FAS\\\\Desktop\\\\alcaraz\\\\InteligenciaArtificial_20120105\\\\Pruebas_cnn\\\\tornado.jpg']\n",
    "\n",
    "# Cargar y redimensionar cada imagen\n",
    "for filepath in filenames:\n",
    "    image = plt.imread(filepath, 0)  # Cargar imagen en escala de grises\n",
    "    image_resized = resize(image, (28, 28), anti_aliasing=True, clip=False, preserve_range=True)\n",
    "    images.append(image_resized)\n",
    "\n",
    "# Convertir lista de imágenes a array de NumPy\n",
    "X = np.array(images, dtype=np.uint8)\n",
    "test_X = X.astype('float32')\n",
    "test_X = test_X / 255.  # Normalizar\n",
    "\n",
    "# Hacer predicciones con el modelo\n",
    "predicted_classes = sport_model.predict(test_X)\n",
    "\n",
    "# Lista de incidentes (debe estar definida)\n",
    "Incidentes = ['asalto', 'incendio', 'inundacion','robo','tornado']  \n",
    "\n",
    "# Mostrar imágenes con el resultado de la predicción\n",
    "for i, img_tagged in enumerate(predicted_classes):\n",
    "    # Obtener el índice de la clase predicha\n",
    "    predicted_index = np.argmax(img_tagged)\n",
    "    # Obtener el nombre del incidente\n",
    "    incident_name = Incidentes[predicted_index]\n",
    "    \n",
    "    # Cargar la imagen original\n",
    "    original_image = cv.imread(filenames[i])\n",
    "    \n",
    "    # Verificar si la imagen se cargó correctamente\n",
    "    if original_image is None:\n",
    "        print(f\"Error al cargar la imagen: {filenames[i]}\")\n",
    "        continue\n",
    "\n",
    "    # Añadir el texto de la predicción en la imagen original\n",
    "    cv.putText(original_image, f\"Prediccion: {incident_name}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    \n",
    "    # Mostrar la imagen\n",
    "    cv.imshow('Imagen', original_image)\n",
    "    \n",
    "    # Esperar a que el usuario presione la tecla ESC para salir\n",
    "    k = cv.waitKey(0)  # Espera indefinidamente hasta que se presione una tecla\n",
    "    if k == 27:  # Tecla ESC\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
