{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:12:59.231970Z",
     "start_time": "2018-11-08T00:12:51.800950Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:13:12.550878Z",
     "start_time": "2018-11-08T00:12:59.234748Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "#from tensorflow.keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, Conv2D\n",
    ")\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar set de Imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.248080Z",
     "start_time": "2018-11-08T00:13:12.553292Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirname = os.path.join(os.getcwd(),'C:\\\\Users\\\\Mermelada\\\\IA_20120105\\\\IA_ESMERALDA-GARCIA-DIAZ-\\\\ProyectoCNN\\\\categorias')\n",
    "imgpath = dirname + os.sep \n",
    "\n",
    "images = []\n",
    "directories = []\n",
    "dircount = []\n",
    "prevRoot=''\n",
    "cant=0\n",
    "\n",
    "print(\"leyendo imagenes de \",imgpath)\n",
    "\n",
    "for root, dirnames, filenames in os.walk(imgpath):\n",
    "    for filename in filenames:\n",
    "        if re.search(\"\\.(jpg|jpeg|png|bmp|tiff)$\", filename):\n",
    "            cant=cant+1\n",
    "            filepath = os.path.join(root, filename)\n",
    "            image = plt.imread(filepath)\n",
    "            if(len(image.shape)==3):\n",
    "                \n",
    "                images.append(image)\n",
    "            b = \"Leyendo imagen...\" + str(cant)\n",
    "            print (b, end=\"\\r\")\n",
    "            if prevRoot !=root:\n",
    "                print(root, cant)\n",
    "                prevRoot=root\n",
    "                directories.append(root)\n",
    "                dircount.append(cant)\n",
    "                cant=0\n",
    "dircount.append(cant)\n",
    "\n",
    "dircount = dircount[1:]\n",
    "dircount[0]=dircount[0]+1\n",
    "print('Directorios leidos:',len(directories))\n",
    "print(\"Imagenes en cada directorio\", dircount)\n",
    "print('suma Total de imagenes en subdirs:',sum(dircount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.269861Z",
     "start_time": "2018-11-08T00:16:45.251786Z"
    }
   },
   "outputs": [],
   "source": [
    "labels=[]\n",
    "indice=0\n",
    "for cantidad in dircount:\n",
    "    for i in range(cantidad):\n",
    "        labels.append(indice)\n",
    "    indice=indice+1\n",
    "print(\"Cantidad etiquetas creadas: \",len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.285925Z",
     "start_time": "2018-11-08T00:16:45.273489Z"
    }
   },
   "outputs": [],
   "source": [
    "Incidentes=[]\n",
    "indice=0\n",
    "for directorio in directories:\n",
    "    name = directorio.split(os.sep)\n",
    "    print(indice , name[len(name)-1])\n",
    "    Incidentes.append(name[len(name)-1])\n",
    "    indice=indice+1\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    print(f\"Image {i} shape: {np.array(img).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.498672Z",
     "start_time": "2018-11-08T00:16:45.290061Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.array(labels)\n",
    "X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
    "\n",
    "\n",
    "\n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos Sets de Entrenamiento y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:45.669596Z",
     "start_time": "2018-11-08T00:16:45.502716Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X,test_X,train_Y,test_Y = train_test_split(X,y,test_size=0.2)\n",
    "print('Training data shape : ', train_X.shape, train_Y.shape)\n",
    "print('Testing data shape : ', test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:49.319746Z",
     "start_time": "2018-11-08T00:16:45.673944Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(train_Y[0]))\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_X[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(test_Y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamos las imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:50.798162Z",
     "start_time": "2018-11-08T00:16:49.322999Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X/255.\n",
    "test_X = test_X/255.\n",
    "plt.imshow(test_X[0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacemos el One-hot Encoding para la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:50.815482Z",
     "start_time": "2018-11-08T00:16:50.800831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_Y[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])\n",
    "print(len(train_X))\n",
    "print(len(train_Y_one_hot))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos el Set de Entrenamiento y Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.218520Z",
     "start_time": "2018-11-08T00:16:50.818992Z"
    }
   },
   "outputs": [],
   "source": [
    "#Mezclar todo y crear los grupos de entrenamiento y testing\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.228116Z",
     "start_time": "2018-11-08T00:16:51.222460Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creamos el modelo de CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.244776Z",
     "start_time": "2018-11-08T00:16:51.238704Z"
    }
   },
   "outputs": [],
   "source": [
    "#declaramos variables con los parámetros de configuración de la red\n",
    "INIT_LR = 1e-3 # Valor inicial de learning rate. El valor 1e-3 corresponde con 0.001\n",
    "epochs = 20 # Cantidad de iteraciones completas al conjunto de imagenes de entrenamiento\n",
    "batch_size = 64 # cantidad de imágenes que se toman a la vez en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.384131Z",
     "start_time": "2018-11-08T00:16:51.252188Z"
    }
   },
   "outputs": [],
   "source": [
    "sport_model = Sequential()\n",
    "sport_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(28,28,3)))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(Flatten())\n",
    "sport_model.add(Dense(32, activation='linear'))\n",
    "sport_model.add(LeakyReLU(alpha=0.1))\n",
    "sport_model.add(Dropout(0.5))\n",
    "sport_model.add(Dense(nClasses, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.401674Z",
     "start_time": "2018-11-08T00:16:51.386676Z"
    }
   },
   "outputs": [],
   "source": [
    "sport_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:16:51.472349Z",
     "start_time": "2018-11-08T00:16:51.406817Z"
    }
   },
   "outputs": [],
   "source": [
    "sport_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=tf.keras.optimizers.SGD(learning_rate=INIT_LR, decay=INIT_LR / 100),metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamos el modelo: Aprende a clasificar imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:49.562522Z",
     "start_time": "2018-11-08T00:16:51.474807Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# este paso puede tomar varios minutos, dependiendo de tu ordenador, cpu y memoria ram libre\n",
    "sport_train = sport_model.fit(train_X, train_label, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:49.676566Z",
     "start_time": "2018-11-08T00:20:49.566203Z"
    }
   },
   "outputs": [],
   "source": [
    "# guardamos la red, para reutilizarla en el futuro, sin tener que volver a entrenar\n",
    "sport_model.save(\"C:\\\\Users\\\\Mermelada\\\\IA_20120105\\\\IA_ESMERALDA-GARCIA-DIAZ-\\\\red.h5\")\n",
    "sport_model.save(\"C:\\\\Users\\\\Mermelada\\\\IA_20120105\\\\IA_ESMERALDA-GARCIA-DIAZ-\\\\red.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluamos la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:54.462929Z",
     "start_time": "2018-11-08T00:20:49.678643Z"
    }
   },
   "outputs": [],
   "source": [
    "test_eval = sport_model.evaluate(test_X, test_Y_one_hot, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:54.474683Z",
     "start_time": "2018-11-08T00:20:54.465378Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_train.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:55.014647Z",
     "start_time": "2018-11-08T00:20:54.479693Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = sport_train.history['accuracy']\n",
    "val_accuracy = sport_train.history['val_accuracy']\n",
    "loss = sport_train.history['loss']\n",
    "val_loss = sport_train.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:58.050602Z",
     "start_time": "2018-11-08T00:20:55.021862Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes2 = sport_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:58.262575Z",
     "start_time": "2018-11-08T00:20:58.052878Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes=[]\n",
    "for predicted_sport in predicted_classes2:\n",
    "    predicted_classes.append(predicted_sport.tolist().index(max(predicted_sport)))\n",
    "predicted_classes=np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:58.272559Z",
     "start_time": "2018-11-08T00:20:58.264703Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendamos de los errores: Qué mejorar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:20:59.822110Z",
     "start_time": "2018-11-08T00:20:58.275464Z"
    }
   },
   "outputs": [],
   "source": [
    "correct = np.where(predicted_classes==test_Y)[0]\n",
    "print(\"Found %d correct labels\" % len(correct))\n",
    "for i, correct in enumerate(correct[0:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_X[correct].reshape(28,28,3), cmap='gray', interpolation='none')\n",
    "    plt.title(\"{}, {}\".format(Incidentes[predicted_classes[correct]],\n",
    "                                                    Incidentes[test_Y[correct]]))\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:21:00.942267Z",
     "start_time": "2018-11-08T00:20:59.829572Z"
    }
   },
   "outputs": [],
   "source": [
    "incorrect = np.where(predicted_classes!=test_Y)[0]\n",
    "print(\"Found %d incorrect labels\" % len(incorrect))\n",
    "for i, incorrect in enumerate(incorrect[0:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(test_X[incorrect].reshape(28,28,3), cmap='gray', interpolation='none')\n",
    "    plt.title(\"{}, {}\".format(Incidentes[predicted_classes[incorrect]],\n",
    "                                                    Incidentes[test_Y[incorrect]]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T00:21:00.968727Z",
     "start_time": "2018-11-08T00:21:00.947262Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_names = [\"Class {}\".format(i) for i in range(nClasses)]\n",
    "print(classification_report(test_Y, predicted_classes, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Incidentes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m sport_model\u001b[38;5;241m.\u001b[39mpredict(test_X)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, img_tagged \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predicted_classes):\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(filenames[i], Incidentes[img_tagged\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(img_tagged))])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Incidentes' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from skimage.transform import resize\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from skimage.transform import resize\n",
    "\n",
    "images=[]\n",
    "# identificar imagenes\n",
    "filenames = ['C:\\\\Users\\\\Mermelada\\\\IA_20120105\\\\IA_ESMERALDA-GARCIA-DIAZ-\\\\osisis.jpg']\n",
    "\n",
    "for filepath in filenames:\n",
    "    image = plt.imread(filepath,0)\n",
    "    image_resized = resize(image, (28, 28),anti_aliasing=True,clip=False,preserve_range=True)\n",
    "    images.append(image_resized)\n",
    "\n",
    "X = np.array(images, dtype=np.uint8) #convierto de lista a numpy\n",
    "test_X = X.astype('float32')\n",
    "test_X = test_X / 255.\n",
    "\n",
    "predicted_classes = sport_model.predict(test_X)\n",
    "\n",
    "for i, img_tagged in enumerate(predicted_classes):\n",
    "    print(filenames[i], Incidentes[img_tagged.tolist().index(max(img_tagged))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 28, 28, 4)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 28, 28, 4), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m test_X \u001b[38;5;241m=\u001b[39m test_X \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m  \u001b[38;5;66;03m# Normalizar\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Hacer predicciones con el modelo\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m sport_model\u001b[38;5;241m.\u001b[39mpredict(test_X)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Lista de incidentes (debe estar definida)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m Incidentes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masalto\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincendio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minundacion\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrobo\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtornado\u001b[39m\u001b[38;5;124m'\u001b[39m]  \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 28, 28, 4)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 28, 28, 4), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from skimage.transform import resize\n",
    "from keras.models import load_model\n",
    "\n",
    "# Cargar el modelo (asegúrate de tener la ruta correcta al modelo)\n",
    "sport_model = load_model('C:\\\\Users\\\\Mermelada\\\\IA_20120105\\\\IA_ESMERALDA-GARCIA-DIAZ-\\\\red.h5')  # Sustituye con la ruta a tu modelo\n",
    "\n",
    "# Inicializar lista para almacenar imágenes redimensionadas\n",
    "images = []\n",
    "# Lista de archivos de imagen\n",
    "filenames = ['C:\\\\Users\\\\Mermelada\\\\IA_20120105\\\\IA_ESMERALDA-GARCIA-DIAZ-\\\\to']\n",
    "\n",
    "# Cargar y redimensionar cada imagen\n",
    "for filepath in filenames:\n",
    "    image = plt.imread(filepath, 0)  # Cargar imagen en escala de grises\n",
    "    image_resized = resize(image, (28, 28), anti_aliasing=True, clip=False, preserve_range=True)\n",
    "    images.append(image_resized)\n",
    "\n",
    "# Convertir lista de imágenes a array de NumPy\n",
    "X = np.array(images, dtype=np.uint8)\n",
    "test_X = X.astype('float32')\n",
    "test_X = test_X / 255.  # Normalizar\n",
    "\n",
    "# Hacer predicciones con el modelo\n",
    "predicted_classes = sport_model.predict(test_X)\n",
    "\n",
    "# Lista de incidentes (debe estar definida)\n",
    "Incidentes = ['asalto', 'incendio', 'inundacion','robo','tornado']  \n",
    "\n",
    "# Mostrar imágenes con el resultado de la predicción\n",
    "for i, img_tagged in enumerate(predicted_classes):\n",
    "    # Obtener el índice de la clase predicha\n",
    "    predicted_index = np.argmax(img_tagged)\n",
    "    # Obtener el nombre del incidente\n",
    "    incident_name = Incidentes[predicted_index]\n",
    "    \n",
    "    # Cargar la imagen original\n",
    "    original_image = cv.imread(filenames[i])\n",
    "    \n",
    "    # Verificar si la imagen se cargó correctamente\n",
    "    if original_image is None:\n",
    "        print(f\"Error al cargar la imagen: {filenames[i]}\")\n",
    "        continue\n",
    "\n",
    "    # Añadir el texto de la predicción en la imagen original\n",
    "    cv.putText(original_image, f\"Prediccion: {incident_name}\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv.LINE_AA)\n",
    "    \n",
    "    # Mostrar la imagen\n",
    "    cv.imshow('Imagen', original_image)\n",
    "    \n",
    "    # Esperar a que el usuario presione la tecla ESC para salir\n",
    "    k = cv.waitKey(0)  # Espera indefinidamente hasta que se presione una tecla\n",
    "    if k == 27:  # Tecla ESC\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
